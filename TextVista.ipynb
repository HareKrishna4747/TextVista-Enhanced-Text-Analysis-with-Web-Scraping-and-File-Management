{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450383a1-3abd-4e88-9376-85d1746f98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_list(file_path, encoding='utf-8'):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            content_list = file.readlines()\n",
    "            content_list = [line.rstrip('\\n').lower() for line in content_list]  # Convert to lowercase\n",
    "            return content_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error decoding file. Try using a different encoding.\")\n",
    "        return None\n",
    "if __name__ == \"__main__\":\n",
    "    positive_file_path = \"Text/positive-words.txt\"\n",
    "    negative_file_path = \"Text/negative-words.txt\"\n",
    "    positive_words = read_file_to_list(positive_file_path, encoding='latin-1')\n",
    "    negative_words = read_file_to_list(negative_file_path, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b628b571-ff01-4169-8180-bb25ed2b1ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the merged stop_words list: 14107\n",
      "Length of the merged positive_words list: 2006\n",
      "Length of the merged negative_words list: 4783\n"
     ]
    }
   ],
   "source": [
    "def append_stop_words(file_path, split_character=\" | \", encoding='latin-1'):\n",
    "    words = read_file_to_list(file_path, encoding=encoding)\n",
    "    if words:\n",
    "        if split_character:\n",
    "            words = [word.split(split_character)[0].lower() for word in words]\n",
    "        stop_words.extend(words)\n",
    "stop_words = []\n",
    "file_paths = [\n",
    "    \"Text/StopWords_Geographic.txt\",\n",
    "    \"Text/StopWords_GenericLong.txt\",\n",
    "    \"Text/StopWords_Generic.txt\",\n",
    "    \"Text/StopWords_Names.txt\",\n",
    "    \"Text/StopWords_DatesandNumbers.txt\",\n",
    "    \"Text/StopWords_Currencies.txt\",\n",
    "    \"Text/StopWords_Auditor.txt\"\n",
    "]\n",
    "for file_path in file_paths:\n",
    "    append_stop_words(file_path)\n",
    "print(\"Length of the merged stop_words list:\", len(stop_words))\n",
    "print(\"Length of the merged positive_words list:\", len(positive_words))\n",
    "print(\"Length of the merged negative_words list:\", len(negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d7186c-51c7-470d-8b23-7574a9bfba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Positive Score': 1, 'Negative Score': -1, 'Polarity Score': 2000000.0, 'Subjectivity Score': 0.0, 'Avg Sentence Length': 5.0, 'Percentage of Complex Words': 1.0, 'Fog Index': 2.4000000000000004, 'Avg Number of Words Per Sentence': 5.0, 'Complex Word Count': 15, 'Syllable Per Word': 37, 'Personal Pronouns Count': 0, 'Avg Word Length': 7.6}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Function to tokenize text and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    return filtered_words\n",
    "\n",
    "# Function to calculate Positive Score\n",
    "def calculate_positive_score(text, positive_words):\n",
    "    positive_words_in_text = [word for word in tokenize_and_remove_stopwords(text) if word in positive_words]\n",
    "    return len(positive_words_in_text)\n",
    "\n",
    "# Function to calculate Negative Score\n",
    "def calculate_negative_score(text, negative_words):\n",
    "    negative_words_in_text = [word for word in tokenize_and_remove_stopwords(text) if word in negative_words]\n",
    "    return -len(negative_words_in_text)\n",
    "\n",
    "# Function to calculate Polarity Score\n",
    "def calculate_polarity_score(positive_score, negative_score):\n",
    "    denominator = (positive_score + negative_score + 0.000001)\n",
    "    return (positive_score - negative_score) / denominator\n",
    "\n",
    "# Function to calculate Subjectivity Score\n",
    "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
    "    denominator = (total_words + 0.000001)\n",
    "    return (positive_score + negative_score) / denominator\n",
    "\n",
    "# Function to calculate Average Sentence Length\n",
    "def calculate_average_sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text) if text else []\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    \n",
    "    # Check if there are sentences to avoid division by zero\n",
    "    if len(sentences) > 0:\n",
    "        return len(words) / len(sentences)\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no sentences\n",
    "\n",
    "# Function to calculate Percentage of Complex Words\n",
    "def calculate_percentage_complex_words(text):\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    \n",
    "    # Check if there are words to avoid division by zero\n",
    "    if len(words) > 0:\n",
    "        complex_words = [word for word in words if len(word) > 2]\n",
    "        return len(complex_words) / len(words)\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no words\n",
    "\n",
    "# Function to calculate Fog Index\n",
    "def calculate_fog_index(average_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Function to calculate Average Number of Words Per Sentence\n",
    "def calculate_average_words_per_sentence(text):\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    sentences = nltk.sent_tokenize(text) if text else []\n",
    "    \n",
    "    # Check if there are sentences to avoid division by zero\n",
    "    if len(sentences) > 0:\n",
    "        return len(words) / len(sentences)\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no sentences\n",
    "\n",
    "# Function to count complex words\n",
    "def count_complex_words(text):\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    complex_words = [word for word in words if len(word) > 2]\n",
    "    return len(complex_words)\n",
    "\n",
    "# Function to count syllables per word\n",
    "def count_syllables_per_word(word):\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "# Function to calculate Personal Pronouns count\n",
    "def calculate_personal_pronouns_count(text):\n",
    "    pronouns = ['i', 'we', 'my', 'ours', 'us']\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    pronoun_count = sum(1 for word in words if word.lower() in pronouns)\n",
    "    return pronoun_count\n",
    "\n",
    "def calculate_average_word_length(text):\n",
    "    words = tokenize_and_remove_stopwords(text)\n",
    "    \n",
    "    # Check if there are words to avoid division by zero\n",
    "    if len(words) > 0:\n",
    "        total_characters = sum(len(word) for word in words)\n",
    "        return total_characters / len(words)\n",
    "    else:\n",
    "        return 0  # Return 0 if there are no words\n",
    "\n",
    "# Function to calculate all metrics together\n",
    "def calculate_all_metrics(text, positive_words, negative_words):\n",
    "    positive_score = calculate_positive_score(text, positive_words)\n",
    "    negative_score = calculate_negative_score(text, negative_words)\n",
    "    total_words = len(tokenize_and_remove_stopwords(text))\n",
    "    average_sentence_length = calculate_average_sentence_length(text)\n",
    "    percentage_complex_words = calculate_percentage_complex_words(text)\n",
    "    fog_index = calculate_fog_index(average_sentence_length, percentage_complex_words)\n",
    "    average_words_per_sentence = calculate_average_words_per_sentence(text)\n",
    "    complex_word_count = count_complex_words(text)\n",
    "    syllables_per_word = sum(count_syllables_per_word(word) for word in tokenize_and_remove_stopwords(text))\n",
    "    personal_pronouns_count = calculate_personal_pronouns_count(text)\n",
    "    average_word_length = calculate_average_word_length(text)\n",
    "\n",
    "    polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
    "    subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
    "\n",
    "    return {\n",
    "        \"Positive Score\": positive_score,\n",
    "        \"Negative Score\": negative_score,\n",
    "        \"Polarity Score\": polarity_score,\n",
    "        \"Subjectivity Score\": subjectivity_score,\n",
    "        \"Avg Sentence Length\": average_sentence_length,\n",
    "        \"Percentage of Complex Words\": percentage_complex_words,\n",
    "        \"Fog Index\": fog_index,\n",
    "        \"Avg Number of Words Per Sentence\": average_words_per_sentence,\n",
    "        \"Complex Word Count\": complex_word_count,\n",
    "        \"Syllable Per Word\": syllables_per_word,\n",
    "        \"Personal Pronouns Count\": personal_pronouns_count,\n",
    "        \"Avg Word Length\": average_word_length\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "text_for_analysis = \"Certainly! I'll define a function that utilizes the previously created functions to calculate all the specified metrics. Additionally, I'll use the provided positive_words and negative_words lists for the positive and negative sets of words.\"\n",
    "\n",
    "metrics_result = calculate_all_metrics(text_for_analysis, positive_words, negative_words)\n",
    "print(metrics_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2065448a-edca-45d2-a683-d1cd84630ca1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching content from URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error fetching content from URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Load the input data\n",
    "input_data = pd.read_excel('Text/Input.xlsx')\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Function to save text to a file\n",
    "def save_text_to_file(url_id, url, title, article_text, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"URL_ID: {url_id}\\n\")\n",
    "        file.write(f\"URL: {url}\\n\")\n",
    "        file.write(f\"Title: {title}\\n\\n\")\n",
    "        file.write(article_text)\n",
    "\n",
    "# Iterate through each article\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Fetch content from the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad requests\n",
    "\n",
    "        # Check if the status code is 404\n",
    "        if response.status_code == 404:\n",
    "            print(f\"Error: URL not found - {url}\")\n",
    "            continue  # Skip the rest of the operations for this URL\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.title.text.strip()\n",
    "        \n",
    "        # Extract text from the webpage (you might need to adjust this based on the HTML structure)\n",
    "        article_text = soup.find('div', class_=\"td-post-content tagdiv-type\")\n",
    "        \n",
    "        if article_text is None:\n",
    "            article_text = soup.find('div', class_=\"tdb_single_content\")\n",
    "        \n",
    "        # Check if the element is found before getting the text\n",
    "        if article_text:\n",
    "            article_text = article_text.get_text(separator='\\n')\n",
    "        else:\n",
    "            article_text = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from URL: {url}\\n{e}\")\n",
    "        title = \"\"\n",
    "        article_text = \"\"  # Set empty text in case of an error\n",
    "\n",
    "    # Save the article text to a file\n",
    "    file_name = f\"Text/{url_id}.txt\"  # Adjust the path as needed\n",
    "    save_text_to_file(url_id, url, title, article_text, file_name)\n",
    "\n",
    "    # Perform textual analysis\n",
    "    metrics_result = calculate_all_metrics(article_text, positive_words, negative_words)\n",
    "    \n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'URL_ID': url_id,\n",
    "        'Title': title,\n",
    "        **metrics_result\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "output_data = pd.DataFrame(results)\n",
    "\n",
    "# Save the output data to 'Output Data Structure.xlsx'\n",
    "output_data.to_excel('Text/Output Data Structure.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc67ecd5-9bcf-4e98-b218-c609b2596942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Polarity Score</th>\n",
       "      <th>Subjectivity Score</th>\n",
       "      <th>Avg Sentence Length</th>\n",
       "      <th>Percentage of Complex Words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Avg Number of Words Per Sentence</th>\n",
       "      <th>Complex Word Count</th>\n",
       "      <th>Syllable Per Word</th>\n",
       "      <th>Personal Pronouns Count</th>\n",
       "      <th>Avg Word Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>Rising IT cities and its impact on the economy...</td>\n",
       "      <td>26</td>\n",
       "      <td>-6</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>6.089744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.835897</td>\n",
       "      <td>6.089744</td>\n",
       "      <td>475</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>6.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>Rising IT Cities and Their Impact on the Econo...</td>\n",
       "      <td>51</td>\n",
       "      <td>-31</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>8.187500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.675000</td>\n",
       "      <td>8.187500</td>\n",
       "      <td>655</td>\n",
       "      <td>1721</td>\n",
       "      <td>0</td>\n",
       "      <td>7.795420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>Internet Demand's Evolution, Communication Imp...</td>\n",
       "      <td>36</td>\n",
       "      <td>-23</td>\n",
       "      <td>4.538461</td>\n",
       "      <td>0.022375</td>\n",
       "      <td>10.192982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.477193</td>\n",
       "      <td>10.192982</td>\n",
       "      <td>581</td>\n",
       "      <td>1654</td>\n",
       "      <td>0</td>\n",
       "      <td>8.390706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>Rise of Cybercrime and its Effect in upcoming ...</td>\n",
       "      <td>35</td>\n",
       "      <td>-74</td>\n",
       "      <td>-2.794872</td>\n",
       "      <td>-0.067826</td>\n",
       "      <td>11.057692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.823077</td>\n",
       "      <td>11.057692</td>\n",
       "      <td>575</td>\n",
       "      <td>1574</td>\n",
       "      <td>0</td>\n",
       "      <td>8.231304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>OTT platform and its impact on the entertainme...</td>\n",
       "      <td>20</td>\n",
       "      <td>-8</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>8.153846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.661538</td>\n",
       "      <td>8.153846</td>\n",
       "      <td>318</td>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "      <td>7.827044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>Due to the COVID-19 the repercussion of the en...</td>\n",
       "      <td>25</td>\n",
       "      <td>-54</td>\n",
       "      <td>-2.724138</td>\n",
       "      <td>-0.057087</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.464000</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>508</td>\n",
       "      <td>1247</td>\n",
       "      <td>0</td>\n",
       "      <td>7.421260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>Impact of COVID-19 pandemic on office space an...</td>\n",
       "      <td>21</td>\n",
       "      <td>-35</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-0.036269</td>\n",
       "      <td>10.157895</td>\n",
       "      <td>0.997409</td>\n",
       "      <td>4.462122</td>\n",
       "      <td>10.157895</td>\n",
       "      <td>385</td>\n",
       "      <td>817</td>\n",
       "      <td>0</td>\n",
       "      <td>6.860104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>Contribution of handicrafts (Visual Arts &amp; Lit...</td>\n",
       "      <td>5</td>\n",
       "      <td>-3</td>\n",
       "      <td>3.999998</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>8.772727</td>\n",
       "      <td>0.989637</td>\n",
       "      <td>3.904946</td>\n",
       "      <td>8.772727</td>\n",
       "      <td>191</td>\n",
       "      <td>450</td>\n",
       "      <td>0</td>\n",
       "      <td>7.305699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>How COVID-19 is impacting payment preferences?...</td>\n",
       "      <td>12</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.666666</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>3.295758</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>232</td>\n",
       "      <td>504</td>\n",
       "      <td>0</td>\n",
       "      <td>6.591667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>How will COVID-19 affect the world of work? - ...</td>\n",
       "      <td>29</td>\n",
       "      <td>-58</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.061181</td>\n",
       "      <td>14.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.325000</td>\n",
       "      <td>14.812500</td>\n",
       "      <td>474</td>\n",
       "      <td>1129</td>\n",
       "      <td>0</td>\n",
       "      <td>7.295359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                              Title  \\\n",
       "0   blackassign0001  Rising IT cities and its impact on the economy...   \n",
       "1   blackassign0002  Rising IT Cities and Their Impact on the Econo...   \n",
       "2   blackassign0003  Internet Demand's Evolution, Communication Imp...   \n",
       "3   blackassign0004  Rise of Cybercrime and its Effect in upcoming ...   \n",
       "4   blackassign0005  OTT platform and its impact on the entertainme...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  Due to the COVID-19 the repercussion of the en...   \n",
       "96  blackassign0097  Impact of COVID-19 pandemic on office space an...   \n",
       "97  blackassign0098  Contribution of handicrafts (Visual Arts & Lit...   \n",
       "98  blackassign0099  How COVID-19 is impacting payment preferences?...   \n",
       "99  blackassign0100  How will COVID-19 affect the world of work? - ...   \n",
       "\n",
       "    Positive Score  Negative Score  Polarity Score  Subjectivity Score  \\\n",
       "0               26              -6        1.600000            0.042105   \n",
       "1               51             -31        4.100000            0.030534   \n",
       "2               36             -23        4.538461            0.022375   \n",
       "3               35             -74       -2.794872           -0.067826   \n",
       "4               20              -8        2.333333            0.037736   \n",
       "..             ...             ...             ...                 ...   \n",
       "95              25             -54       -2.724138           -0.057087   \n",
       "96              21             -35       -4.000000           -0.036269   \n",
       "97               5              -3        3.999998            0.010363   \n",
       "98              12              -3        1.666666            0.037500   \n",
       "99              29             -58       -3.000000           -0.061181   \n",
       "\n",
       "    Avg Sentence Length  Percentage of Complex Words  Fog Index  \\\n",
       "0              6.089744                     1.000000   2.835897   \n",
       "1              8.187500                     1.000000   3.675000   \n",
       "2             10.192982                     1.000000   4.477193   \n",
       "3             11.057692                     1.000000   4.823077   \n",
       "4              8.153846                     1.000000   3.661538   \n",
       "..                  ...                          ...        ...   \n",
       "95            10.160000                     1.000000   4.464000   \n",
       "96            10.157895                     0.997409   4.462122   \n",
       "97             8.772727                     0.989637   3.904946   \n",
       "98             7.272727                     0.966667   3.295758   \n",
       "99            14.812500                     1.000000   6.325000   \n",
       "\n",
       "    Avg Number of Words Per Sentence  Complex Word Count  Syllable Per Word  \\\n",
       "0                           6.089744                 475               1060   \n",
       "1                           8.187500                 655               1721   \n",
       "2                          10.192982                 581               1654   \n",
       "3                          11.057692                 575               1574   \n",
       "4                           8.153846                 318                780   \n",
       "..                               ...                 ...                ...   \n",
       "95                         10.160000                 508               1247   \n",
       "96                         10.157895                 385                817   \n",
       "97                          8.772727                 191                450   \n",
       "98                          7.272727                 232                504   \n",
       "99                         14.812500                 474               1129   \n",
       "\n",
       "    Personal Pronouns Count  Avg Word Length  \n",
       "0                         0         6.894737  \n",
       "1                         0         7.795420  \n",
       "2                         0         8.390706  \n",
       "3                         0         8.231304  \n",
       "4                         0         7.827044  \n",
       "..                      ...              ...  \n",
       "95                        0         7.421260  \n",
       "96                        0         6.860104  \n",
       "97                        0         7.305699  \n",
       "98                        0         6.591667  \n",
       "99                        0         7.295359  \n",
       "\n",
       "[100 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed291c5-ed62-4f5b-8d13-fc410f8bc695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06ebe24-2a2e-4bbe-bc4b-e0d82557c00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
